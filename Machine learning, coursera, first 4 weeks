import numpy as np
import pandas
import collections
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import sklearn.preprocessing
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_boston
from sklearn.linear_model import Perceptron
import sklearn.metrics
import sklearn.svm
from sklearn import datasets
import sklearn.feature_extraction.text
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
import math
from sklearn.feature_extraction import DictVectorizer
from scipy.sparse import hstack
from sklearn.decomposition import PCA

# Z = np.array([[4, 5, 0],
#              [1, 9, 3],
#              [5, 1, 1],
#              [3, 3, 3],
#              [9, 9, 9],
#              [4, 7, 1]])
#
#
# r = np.sum(Z, axis=1)
# print(np.nonzero(r > 10))
#
# data = pandas.read_csv('titanic.csv', index_col='PassengerId')
# print(data.head())
#
# SibSp = data['SibSp']
# Parch = data['Parch']
#
# print(SibSp.corr(Parch, method='pearson'))
#
# a = np.array(data['Sex'])
# print(collections.Counter(a))
# print(type(a))
#
# Age = data['Age']
# print(np.mean(Age))
# A = [5, 7, 11]
# print(Age.median())
#
# name = data['Name']
# print(name)
# woman_names = []
# for str in name:
#     str = str.split()
#     if str[1] == 'Miss.' or str[1] == 'Mrs.':
#         woman_names.append(str[2])
#         if str[2] == 'William':
#             print(str)
#
# print(collections.Counter(woman_names))
# data = data[~np.isnan(data['Age'])]
# data.Sex.replace(to_replace=dict(female=1, male=0), inplace=True)
# left_features = data[['Pclass', 'Fare', 'Age', 'Sex']]
#
# y = data['Survived']
#
# print(type(data))
# d = {'col1': [1, 2], 'col2': [3, 4]}
# df = pandas.DataFrame(data=d)
# print(type(df))
#
# print(left_features)
#
# clf = DecisionTreeClassifier(random_state=241)
# clf.fit(left_features, y)
#
# importances = clf.feature_importances_
# print(importances)
#
# print(left_features)

### WEEK 2

# wine_data = pandas.read_csv('wine.data', header=None)
# y = wine_data[0]
# X = wine_data.iloc[:, 1:]
# X = sklearn.preprocessing.scale(X)
#
# kf = KFold(n_splits=5, shuffle=True, random_state=42)
#
# neigh = KNeighborsClassifier(n_neighbors=1)
#
# tList = cross_val_score(neigh, X, y, cv=kf)
#
# averaging = np.sum(tList)
# max_k = 1
# max_aver = averaging
#
#
# for k in range(2, 51):
#     neigh = KNeighborsClassifier(n_neighbors=k)
#     tList = cross_val_score(neigh, X, y, cv=kf)
#     averaging = np.sum(tList)
#
#     if averaging > max_aver:
#         max_aver = averaging
#         max_k = k
#
# print(max_k, max_aver / 5)
#
# print(X)

#WEEK 2 question about metric

# boston = load_boston()
#
# boston['data'] = sklearn.preprocessing.scale(boston['data'])
#
#
# kf = KFold(n_splits=5, shuffle=True, random_state=42)
#
#
# for p in np.linspace(1, 10, num=200):
#     neigh = KNeighborsRegressor(n_neighbors=5, weights='distance', p = p)
#     tList = cross_val_score(neigh, boston['data'], boston['target'], scoring='neg_mean_squared_error', cv=kf)
#     averaging = np.sum(tList)
#
#     if p == 1:
#         max_val = averaging
#         max_p = p
#
#     else:
#         if averaging > max_val:
#             max_val = averaging
#             max_p = p
#
# print(max_p)
# print(max_val / 5)

###Perceptron - классификатор! В cross_val_score estimator - классификатор

#WEEK 2 final exercise

# clf = Perceptron(random_state=241, max_iter=5, tol=None)
#
# learning_data = pandas.read_csv('perceptron-train.csv', header=None)
# test_data = pandas.read_csv('perceptron-test.csv', header=None)
#
# y_training = learning_data[0]
# X_training = learning_data.iloc[:, 1:]
#
# y_test = test_data[0]
# X_test = test_data.iloc[:, 1:]
#
# clf.fit(X_training, y_training)
# predictions_before = clf.predict(X_test)
# first_result = sklearn.metrics.accuracy_score(y_test, predictions_before)
#
# ##нормализация выборок трэин и тест
# #насчет нормализации: трэин делаешь с fit_transform, он запоминает отклонения и тестовую делаешь с
# #обычным transform (там применяеются данные, полученные с fir_transform)
# scaler = sklearn.preprocessing.StandardScaler()
# X_train_scaled = scaler.fit_transform(X_training)  ##Возможно тут и ниже нужно transform вместо fit_transform
# X_test_scaled = scaler.transform(X_test)
#
#
# clf_2 = Perceptron(random_state=241, max_iter=5, tol=None)
#
# clf_2.fit(X_train_scaled, y_training)
# predict_final = clf_2.predict(X_test_scaled)
# second_result = sklearn.metrics.accuracy_score(y_test, predict_final)
# print(second_result - first_result)

#WEEK 3 SVM

# data = pandas.read_csv('svm-data.csv', header=None)
# y = data[0]
# X = data.iloc[:, 1:]
#
# clf = sklearn.svm.SVC(C = 100000, random_state=241, kernel='linear')
# clf.fit(X, y)
# print(y)

#WEEK 3 second exer SVM

# newsgroups = datasets.fetch_20newsgroups(
#     subset='all',
#     categories=['alt.atheism', 'sci.space']
# )
# #преобразование текстовых данных к числовым (для применения методов МЛ):
# vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()
# newsgroups.data = vectorizer.fit_transform(newsgroups.data)
#
#
# #clf - классификатор/estimator - т.е. алгоритм, который по признакам относит объект к какому-то классу
# #cv - cross-validation. Способ проверки результатов алгоритма на тестовых значениях
# cv = KFold(n_splits=5, shuffle=True, random_state=241)
# clf = sklearn.svm.SVC(random_state=241, kernel='linear', C = 1)
#
# grid = {'C': np.power(10.0, np.arange(-5, 6))}
#
# clf.fit(newsgroups.data, newsgroups.target)
#
# word_indexes = np.argsort(np.abs(clf.coef_.toarray()[0]))[-10:]
#
# feature_mapping = vectorizer.get_feature_names()
#
# for index in word_indexes:
#     print(feature_mapping[index])
#
# print(clf.coef_)
# print(clf.coef_.toarray()[0])
# print(word_indexes)

# feature_mapping = vectorizer.get_feature_names()

#GridSearch - как я понимаю, поиск оптимального параметра (из какого-то множества)
# на вход получаем классификатор, кросс валидацию (с ее параметрами),
# scoring='accuracy' отвечает за то, что наша мера - доля правильных ответов
# потом кормим ей тестовую выборку с правильными ответами
# в gs.grid_scores_ хранятся данные определенного типа (хз какого типа, кстати)
# с методами mean_validation и parameters, которые говорят о мере правильных ответов и о том,
# при каком параметре такие данные получили
# работает этот грид сеч достаточно медленно. Мб из-за того, что тут огромный текст, хз


# gs = GridSearchCV(clf, grid, scoring='accuracy', cv=cv)
# gs.fit(newsgroups.data, newsgroups.target)
#
# for a in gs.grid_scores_:
#     print(a.mean_validation_score) # — оценка качества по кросс-валидации
#     print(a.parameters) # — значения параметров

# мусорный код. По сути, (поидее) должен делать то же, что и GridSearch, но медленее

# tList = cross_val_score(clf, newsgroups.data, newsgroups.target, scoring='neg_mean_squared_error', cv=kf)
# max = np.sum(tList)
# optimal = 0
#
# for i in range(1, 12):
#     clf = sklearn.svm.SVC(C=(10 ** (-5) * 10 ** i), random_state=241, kernel='linear')
#     tList = cross_val_score(clf, newsgroups.data, newsgroups.target, scoring='neg_mean_squared_error', cv=kf)
#
#     averaging = np.sum(tList)
#
#     if averaging > max:
#         max = averaging
#         optimal = i
#
# optimal_c = 10 ** (-5) * 10 ** optimal

# WEEK 3 part 2
# чей-то код с курсеры, для вычисления промежуточного слагаемого в град спуске
# def delta(X, y, w, c, k):
#     dp = np.einsum('ij,j->i',X,w)
#     coeff = 1-(1/(1+np.exp(-y*dp)))
#     a = k/coeff.shape[0]*(np.einsum('ij,i,i->j',X,y,coeff))
#     w = w + a - k*c*w
#     return w



# data = pandas.read_csv('data-logistic.csv', header=None)
# y = data[0]
# X = data.iloc[:, 1:]

# w_1_reg = 0.028561965517012657
# w_2_reg = 0.024783655436404747
# w_1 = 0.28780442348832036
# w_2 = 0.09198995719421853
#
# y_regular = list(y)
# y_trivial = list(y)
# for i in range(len(y)):
#     y_regular[i] = 1 / (1 + np.exp(-w_1_reg * X[1][i] - w_2_reg * X[2][i]))
#     y_trivial[i] = 1 / (1 + np.exp(-w_1 * X[1][i] - w_2 * X[2][i]))
#
# print(sklearn.metrics.roc_auc_score(y, y_regular))
# print(sklearn.metrics.roc_auc_score(y, y_trivial))

# Ниже, реализован градиентный спуск для поиска оптимальных параметров (не уверен что верно)
# он отрабатывается не очень быстро, поэтому я его закомментил и ниже привожу оптимальные параметры
# которые я получил на нем
# логистическая регуляризованная регрессия: w_1 = 0.28781162047177566, w_2 = 0.09208361100689844
# обычная логистическая: w_1 = 0.5221348411851868, w_2 = 0.2848575555647193
# # реализую градиентный спуск для регуляризованной логистической регрессии? (поидее это для регуляризованной)
# # начальный вектор - (0, 0)
#

# w_1_old = 0.0
# w_2_old = 0.0
#
# w_1 = 1
# w_2 = 1
#
# k = 0.1
# C = 10.0
#
# l = len(y)
#
# sum_1 = 0.0
# sum_2 = 0.0
#
# # тут ниже идет алгоритм градиентного спуска для регуляризованной логистической регрессии
# t = 0
# while True:
#
#     for i in range(l):
#         # тут будет подсчет суммы в град спуске
#         sum_1 = sum_1 + y[i] * X[1][i] * (1 - 1.0 / (1 + np.exp(-y[i] * (w_1_old * X[1][i] + w_2_old * X[2][i]))))
#         sum_2 = sum_2 + y[i] * X[2][i] * (1 - 1.0 / (1 + np.exp(-y[i] * (w_1_old * X[1][i] + w_2_old * X[2][i]))))
#
#     w_1 = w_1_old + sum_1 * k / (l + 1) - k * C * w_1_old
#     w_2 = w_2_old + sum_2 * k / (l + 1) - k * C * w_2_old
#
#     if t == 0:
#         print(w_1, w_2)
#         print(sum_1 * k / (l + 1))
#         print(sum_2 * k / (l + 1))
#
#     t = 1
#
#     if math.sqrt((w_1 - w_1_old) ** 2 + (w_2 - w_2_old) ** 2) < 10 ** (-5):
#         break
#
#     w_1_old = w_1
#     w_2_old = w_2
#
# print(w_1, w_2)
#
# # поидее ниже код для обычной логистической регрессии (т.е. С = 0, если я все правильно понял)

# w_1_old = 0
# w_2_old = 0
#
# w_1 = 1
# w_2 = 1
#
# k = 0.1
#
# l = len(y)
#
# sum_1 = 0
# sum_2 = 0
#
# t = 0
#
# while True:
#
#     for i in range(l):
#         # тут будет подсчет суммы в град спуске
#         fraction = 1 / (1 + np.exp(-y[i] * (w_1_old * X[1][i] + w_2_old * X[2][i])))
#
#         sum_1 += y[i] * X[1][i] * (1 - fraction)
#         sum_2 += y[i] * X[2][i] * (1 - fraction)
#
#     w_1 = w_1_old + sum_1 * k / (l + 1)
#     w_2 = w_2_old + sum_2 * k / (l + 1)
#
#     if t == 0:
#         print(w_1, w_2)
#
#
#     if math.sqrt((w_1 - w_1_old) ** 2 + (w_2 - w_2_old) ** 2) < 10 ** (-5) or t >= 10000:
#         break
#
#     w_1_old = w_1
#     w_2_old = w_2
#
#     t += 1
#
#     print(t)
#
# print(w_1, w_2)

# X_ = data.values[:, 1:]
# y_ = data.values[:, :1].T[0]
#
# def sigmoid(t):
#     return 1.0 / (1 + np.exp(t))
#
# def distance(a, b):
#     return np.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)
#
# def log_regression(X, y, k, w, C, epsilon, max_iter):
#     w1, w2 = w
#     for i in range(max_iter):
#         w1new = w1 + k * np.mean(y * X[:, 0] * (1 - (1. / (1 + np.exp(-y * (w1 * X[:, 0] + w2 * X[:, 1])))))) - k * C * w1
#         w2new = w2 + k * np.mean(y * X[:, 1] * (1 - (1. / (1 + np.exp(-y * (w1 * X[:, 0] + w2 * X[:, 1])))))) - k * C * w2
#
#         if distance((w1new, w2new), (w1, w2)) < epsilon:
#             break
#
#         if i == 0:
#             print(w1new, w2new)
#             print(k * np.mean(y * X[:, 0] * (1 - (1. / (1 + np.exp(-y * (w1 * X[:, 0] + w2 * X[:, 1])))))))
#             print(k * np.mean(y * X[:, 1] * (1 - (1. / (1 + np.exp(-y * (w1 * X[:, 0] + w2 * X[:, 1])))))))
#
#         w1, w2 = w1new, w2new
#
#     return (w1, w2)
#
# print(log_regression(X_, y_, 0.1, [0.0, 0.0], 10, 0.00001, 10000))
#
# data = pandas.read_csv('classification.csv')
# y_true = data['true']
# y_pred = data['pred']

# accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)
# precision = sklearn.metrics.precision_score(y_true, y_pred)
# recall = sklearn.metrics.recall_score(y_true, y_pred)
# F_measure = sklearn.metrics.f1_score(y_true, y_pred)
#
# TP, FP, FN, TN = [0, 0, 0, 0]
#
# print(accuracy, precision, recall, F_measure)
#
# data_scores = pandas.read_csv('scores.csv')
# y_data_true = data_scores['true']
#
# score_logreg = data_scores['score_logreg']
# score_svm = data_scores['score_svm']
# score_knn = data_scores['score_knn']
# score_tree = data_scores['score_tree']
#
# roc_logreg = sklearn.metrics.roc_auc_score(y_data_true, score_logreg)
# roc_svm = sklearn.metrics.roc_auc_score(y_data_true, score_svm)
# roc_knn = sklearn.metrics.roc_auc_score(y_data_true, score_knn)
# roc_tree = sklearn.metrics.roc_auc_score(y_data_true, score_tree)
#
# print(data_scores)
#
# print(roc_logreg, roc_svm, roc_knn, roc_tree) # svm - best
# logreg_curve = sklearn.metrics.precision_recall_curve(y_true, score_logreg)
# svm_curve = sklearn.metrics.precision_recall_curve(y_true, score_svm)
# knn_curve = sklearn.metrics.precision_recall_curve(y_true, score_knn)
# tree_curve = sklearn.metrics.precision_recall_curve(y_true, score_tree)
#
# print(logreg_curve)

#WEEK 4
#
# data_train = pandas.read_csv('salary-train.csv')
# data_test = pandas.read_csv('salary-test-mini.csv')
# # Предобработка ТЕКСТОВЫХ данных (перевод в нижний регистр и удаление всех символов кроме цифробукв
# data_train['FullDescription'] = data_train['FullDescription'].str.lower()
# data_train['LocationNormalized'] = data_train['LocationNormalized'].str.lower()
# data_train['ContractTime'] = data_train['ContractTime'].str.lower()
#
# data_train['FullDescription'] = data_train['FullDescription'].replace('[^a-zA-Z0-9]', ' ', regex = True)
# data_train['LocationNormalized'] = data_train['LocationNormalized'].replace('[^a-zA-Z0-9]', ' ', regex = True)
# data_train['ContractTime'] = data_train['ContractTime'].replace('[^a-zA-Z0-9]', ' ', regex = True)
#
# # sklearn.feature_extraction.text.TfidfVectorizer отвечает за текстовые TF-IDF признаки
#
# enc = DictVectorizer()
# vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=5)
#
# X_description = vectorizer.fit_transform(data_train['FullDescription'])
#
# data_train['LocationNormalized'].fillna('nan', inplace=True)
# data_train['ContractTime'].fillna('nan', inplace=True)
#
# X_train_categ = enc.fit_transform(data_train[['LocationNormalized', 'ContractTime']].to_dict('records'))
#
# X = hstack([X_description, X_train_categ])
#
# y_train = np.array(data_train['SalaryNormalized'])
#
# y_train = y_train.astype(np.float)
#
# from sklearn.linear_model import Ridge
#
# clf = Ridge(alpha=1.0, random_state=241)
#
# clf.fit(X, y_train)
#
# X_test_vect = vectorizer.transform(data_test['FullDescription'])
# X_test_categ = enc.transform(data_test[['LocationNormalized', 'ContractTime']].to_dict('records'))
# X_test = hstack([X_test_vect, X_test_categ])
# Ts = clf.predict(X_test)
#
# print(Ts)
#
# # X_test_categ = enc.transform(data_test[['LocationNormalized', 'ContractTime']].to_dict('records'))
# # newsgroups = datasets.fetch_20newsgroups(
# #     subset='all',
# #     categories=['alt.atheism', 'sci.space']
# # )
# # #преобразование текстовых данных к числовым (для применения методов МЛ):
# # vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()
# # newsgroups.data = vectorizer.fit_transform(newsgroups.data)

# WEEK 4 exer 2

# data_close = pandas.read_csv('close_prices.csv')
# djia_index = pandas.read_csv('djia_index.csv')
# djia_index = djia_index.iloc[:, 1:]
#
# pca = PCA(n_components=10)
#
# pca.fit(data_close.iloc[:, 1:])
#
#
#
# # components = np.asmatrix(pca.transform(data_close.iloc[:, 1:])).transpose()
# # components = np.array(components[0])[0]
# # djia_index = np.array((np.asmatrix(djia_index).transpose()))[0]
#
# # Pearson = np.corrcoef(components, djia_index)
#
#
# # print(pca.explained_variance_ratio_)
# # print(data_close.iloc[:, 1:])
#
# # решение третьего вопроса:
#
# data_close = data_close.iloc[:, 1:]
# # print(data_close[[0]])
# components = pca.components_[0]
# max = components[0]
# number = 0
# for i in range(30):
#     if components[i] > max:
#         max = components[i]
#         number = i
# print(data_close[[number]])
